---
layout: page-sidenav
group: "Chapter. 5"
title: "4. The Hessian Matrix"
---

- 지금까지 우리는 backprop 기법을 이용하여 에러 함수에 대한 \\( {\bf w} \\) 1차 미분값을 계산할 수 있었다.
- 이를 확장하여 2차 미분값인 Hessian 행렬을 계산하는 방법을 살펴보도록 하자.

$$\frac{\partial^2 E}{\partial w_{ji} \partial w_{lk} } \qquad{(5.78)}$$

- 여기서 \\( i, j \in \\\{ 1, ..., W \\\} \\) 이고 \\( W \\) 는 모든 weight 와 bias를 포함한다.
- 이 때 각각의 2차 미분 값을 \\( H_{ij} \\) 로 표기하고 이것으로 만들어지는 행렬을 헤시안(Hessian) 행렬 \\( {\bf H} \\) 라고 정의한다.
- 신경망에서 헤시안 행렬은 중요하게 여겨진다.
    - 일부 비선형 최적화 알고리즘에서 에러 곡면의 2차 미분 값을 사용한다.
    - 이미 학습이 완료된 신경망에 조금 변경된 데이터를 입력으로 주고 빠르게 재학습 시킬 때 사용된다.
    - 'pruning' 일고리즘의 일부로 *least significant weights* 를 식별할 때 헤시안 역행렬이 사용된다.
    - 베이지안 신경망을 위한 라플라스 근사식에서 중요한 역할을 차지한다. (5.7절 참고)
- 헤시안 행렬의 연산량은 매우 높다.
    - 신경망에 \\( W \\) 개의 weight 가 존재한다면 헤시안은 \\( W \times W \\) 행렬이 된다.
    - 따라서 연산량은 입력 샘플당 \\( O(W^2) \\) 이 필요하다.

## 5.4.1. 대각 근사 ( Diagonal approximation)


